{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **load pickle data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickle data\n",
    "import pickle\n",
    "\n",
    "trainX = pickle.loads(open(\"C:/Users/AbsSayem/.vscode/computer_vision/dataset/HARDataset1/trainX.pickle\", \"rb\").read())\n",
    "trainY = pickle.loads(open(\"C:/Users/AbsSayem/.vscode/computer_vision/dataset/HARDataset1/trainY.pickle\", \"rb\").read())\n",
    "testX  = pickle.loads(open(\"C:/Users/AbsSayem/.vscode/computer_vision/dataset/HARDataset1/testX.pickle\", \"rb\").read())\n",
    "testY  = pickle.loads(open(\"C:/Users/AbsSayem/.vscode/computer_vision/dataset/HARDataset1/testY.pickle\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"Boxing\", \"HandClapping\", \"HandWaving\", \"Jogging\", \"Running\", \"Standing\", \"Walking\"]\n",
    "print(CATEGORIES[trainY[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images from pickle data\n",
    "import matplotlib.pyplot as plt\n",
    "# show a single image\n",
    "#plt.imshow(trainX[0], cmap='gray')\n",
    "\n",
    "# show multiple images\n",
    "CATEGORIES = [\"Boxing\", \"HandClapping\", \"HandWaving\", \"Jogging\", \"Running\", \"Standing\", \"Walking\"]\n",
    "num_images = 10\n",
    "# grid for subplots\n",
    "rows, cols = 2, 5\n",
    "# make a grid\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(13,4))\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# read and place images in the plot\n",
    "for i in range(min(num_images, rows * cols)):\n",
    "    axes[i].imshow(trainX[i], cmap='gray')  # Use 'cmap=None' for color images\n",
    "    axes[i].set_title(f'{CATEGORIES[trainY[i]]}')\n",
    "    #axes[i].axis('off')  # Optional: Turn off axis labels\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the frames\n",
    "for iter in range(len(trainX)):    # len(trainX)=45952\n",
    "    # when iter==0: it gets the array of image 0\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`here we are able to access the frames. we will return back here after extracting foreground(human_shape) and getting the gait data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**foreground extraction**\n",
    "* to segment and seperate foreground we will use OpenCV and GrabCut.\n",
    "* how the GrabCut algorithm works?- by repetating the following steps-\n",
    "    1. Estimate the color distribution of the foreground and background via a Gaussian Mixture Model (GMM)\n",
    "    2. Construct a Markov random field over the pixels labels (i.e., foreground vs. background)\n",
    "    3. Apply a graph cut optimization to arrive at the final segmentation\n",
    "* OpenCV has an implementation of GrabCut via the `cv2.grabCut` function\n",
    "\n",
    "**opencv grabcut: foreground segmentation and extraction - let's learn about opencv grabcut function:**\n",
    "* `grabCut(img, mask, rect, bgdModel, fgdModel, iterCount[, model]) -> mask, bgdModel, fgdModel`\n",
    "* `input arguments`:\n",
    "    * `img`: the input image, (unsigned 8-bit integer in BGR (3) channel ordering)\n",
    "    * `mask`: the input/output mask (a single-channel unsigned 8-bit integer).\n",
    "        * `cv2.GC_INIT_WITH_RECT`: the bounding box initialization, mask is initiated automatically\n",
    "        * `cv2.GC_INIT_WITH_MASK`: the mask initialization\n",
    "    * `rect`: the bounding box rectangle that contains the region that we want to segment, only used when we set the `mode` to `cv2.GC_INIT_WITH_MASK`\n",
    "    * `bgModel`: temporary array used by GrabCut internally when modeling the background (no need external manipulation)\n",
    "    * `fgModel`: temporary array used by GrabCut internally when modeling the foreground (no need external manipulation)\n",
    "    * `mode`: either `cv2.GC_INIT_WITH_RECT` or `cv2.GC_INIT_WITH_MASK`, depends on what I am initializing\n",
    "* `output arguments`:\n",
    "    * `mask`: the output mask after applying GrabCut\n",
    "    * `bgModel`: the temporary array used to model background (ignorable)\n",
    "    * `fgModel`: the temporary array used to model foreground (ignorable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**implement grabcut initializer (bounding_box vs mask)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bounding box`\n",
    "* specify the bounding box is then used by the grabcut function to segment the foreground\n",
    "* bounding box can be generated by -\n",
    "    * manually labelling the (x,y)-coordinates of the bounding box\n",
    "    * applying a `Haar Cascade` algorithm\n",
    "    * using `HOG` + `Linear SVM` to detect the object\n",
    "    * using dl-based object detector, like- `Faster R-CNN`, `SSDs`, `YOLO` etc\n",
    "* `we will use haar cascade algorithm, because - it is suitable for single activity, also very fast.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding box using haar casecade\n",
    "def bounding_box(img):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cv2\n",
    "    harcascade = \"models/haarcascade_fullbody.xml\"\n",
    "    fullcascade = cv2.CascadeClassifier(harcasecade)\n",
    "    #img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    full_body = fullcascade.detectMultiScale(img, 1.1, 4)\n",
    "    for (x,y,w,h) in full_body:\n",
    "        cv2.ractangle(img, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x,y,w,h) \u001b[38;5;129;01min\u001b[39;00m face:\n\u001b[0;32m     15\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(img, (x,y), (x\u001b[38;5;241m+\u001b[39mw, h\u001b[38;5;241m+\u001b[39mh), (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "harcascade = \"models/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640)  # width\n",
    "cap.set(4,480)  # height\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    facecascade = cv2.CascadeClassifier(harcascade)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face = facecascade.detectMultiScale(img_gray, 1.1,4)\n",
    "    for (x,y,w,h) in face:\n",
    "        cv2.rectangle(img, (x,y), (x+w, h+h), (0,0,255), 2)\n",
    "    #cv2.imshow(\"Face\", img)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check using a frame\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "img1 = trainX[0]\n",
    "print(img1.shape)\n",
    "#plt.imshow(img1)\n",
    "#bounding_box(img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`masks`\n",
    "* using mask, we can supply the approximate segmentation of the object in the image. grabcut can then iteratively apply graph cuts to improve the segmentation and extract the foreground from the image.\n",
    "* the masks could be generated by -\n",
    "    * manually creating in photo editing software, like- `photoshop`\n",
    "    * applying ip operations, like- `thresholding`, `edge-detection`, `contour filtering`\n",
    "    * utilizing dl-based segmentation nets, like- `Mask R-CNN`, `U-NET`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks \n",
    "def masks():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
